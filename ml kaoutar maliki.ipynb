{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb082f4",
   "metadata": {},
   "source": [
    "# Credit Risk Project — End-to-end Notebook\n",
    "\n",
    "This notebook documents and runs the full workflow for the credit risk project:\n",
    "\n",
    "- Load dataset\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Data cleaning and outlier handling\n",
    "- Model training & evaluation (LogReg, RandomForest, GradientBoosting)\n",
    "- Save best model artifact and run sample predictions\n",
    "\n",
    "Notes:\n",
    "- This notebook uses project helper functions in `src/` (e.g., `data_io`, `cleaning`, `modeling`).\n",
    "- Update paths or configs as needed to suit your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d116c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup: imports and environment\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Project helpers\n",
    "from src.data_io import read_dataset_from_path\n",
    "from src.constants import DEFAULT_DATASET_PATH, TARGET_COL, ARTIFACTS_DIR\n",
    "from src.cleaning import CleaningConfig, clean_dataframe, infer_column_types\n",
    "from src.modeling import TrainConfig, train_and_evaluate, save_artifact, load_artifact\n",
    "\n",
    "# Quick versions check\n",
    "import sklearn, joblib\n",
    "print(f\"Python: {sys.version.splitlines()[0]}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"joblib: {joblib.__version__}\")\n",
    "\n",
    "# plotting defaults\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = DEFAULT_DATASET_PATH\n",
    "ARTIFACTS_DIR = ARTIFACTS_DIR\n",
    "print(f\"Dataset path: {DATA_PATH}\")\n",
    "print(f\"Artifacts dir: {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc9c4e",
   "metadata": {},
   "source": [
    "## 2) Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72696e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = read_dataset_from_path(DATA_PATH)\n",
    "print(\"Dataset loaded:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01afde4",
   "metadata": {},
   "source": [
    "## 3) Exploratory Data Analysis (EDA)\n",
    "\n",
    "Check basic distribution, target balance and quick diagnostics for numeric/categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34764a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic summary\n",
    "print(df.info())\n",
    "\n",
    "# Target distribution\n",
    "print('\\nTarget distribution:')\n",
    "print(df[TARGET_COL].value_counts(dropna=False))\n",
    "\n",
    "# Numeric summary\n",
    "display(df.describe(include=[\"number\"]).T)\n",
    "\n",
    "# Categorical top values\n",
    "display(df.describe(include=[\"object\", \"category\"]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922ef0a",
   "metadata": {},
   "source": [
    "## 4) Data cleaning\n",
    "\n",
    "Use project `cleaning` helpers to handle missing values, duplicates, and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer numeric/categorical columns\n",
    "numeric_cols, categorical_cols = infer_column_types(df, TARGET_COL)\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "\n",
    "# Configure cleaning\n",
    "cfg = CleaningConfig(\n",
    "    target_col=TARGET_COL,\n",
    "    numeric_missing=\"median\",\n",
    "    categorical_missing=\"mode\",\n",
    "    drop_duplicates=True,\n",
    "    outlier_method=\"zscore\",  # 'none' | 'zscore' | 'mean_std'\n",
    "    outlier_cols=numeric_cols,  # you can restrict to specific important numeric cols\n",
    "    zscore_threshold=3.0,\n",
    "    mean_std_k=3.0,\n",
    ")\n",
    "\n",
    "cleaned_df, clean_report = clean_dataframe(df, cfg)\n",
    "print(\"Cleaning report:\")\n",
    "for k, v in clean_report.items():\n",
    "    print(k, \":\", v)\n",
    "\n",
    "print(\"After cleaning shape:\", cleaned_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2da31",
   "metadata": {},
   "source": [
    "## 5) Modeling\n",
    "\n",
    "Train multiple models and compare evaluation metrics. Uses `train_and_evaluate` helper from `src/modeling.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"LogReg\", \"RandomForest\", \"GradientBoosting\"]\n",
    "results = {}\n",
    "\n",
    "for m in models:\n",
    "    print(f\"Training: {m}\")\n",
    "    train_cfg = TrainConfig(target_col=TARGET_COL, test_size=0.2, random_state=42, model_name=m)\n",
    "    pipe, metrics = train_and_evaluate(\n",
    "        cleaned_df,\n",
    "        numeric_cols=numeric_cols,\n",
    "        categorical_cols=categorical_cols,\n",
    "        cfg=train_cfg,\n",
    "    )\n",
    "    results[m] = (pipe, metrics)\n",
    "    print(f\" -> accuracy: {metrics['accuracy']:.4f}, f1: {metrics['f1']:.4f}, roc_auc: {metrics.get('roc_auc')}\")\n",
    "\n",
    "# Summarize\n",
    "summary = pd.DataFrame([\n",
    "    {\"model\": m, **r[1]}\n",
    "    for m, r in results.items()\n",
    "])\n",
    "summary = summary.set_index(\"model\")[[\"accuracy\", \"f1\", \"precision\", \"recall\", \"roc_auc\"]]\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90d211",
   "metadata": {},
   "source": [
    "## 6) Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d59afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best by F1 score\n",
    "best_name, (best_pipe, best_metrics) = max(results.items(), key=lambda kv: kv[1][1][\"f1\"])\n",
    "print(f\"Best model: {best_name} — f1: {best_metrics['f1']:.4f}\")\n",
    "\n",
    "artifact_path = ARTIFACTS_DIR / \"credit_risk_model_notebook.joblib\"\n",
    "save_artifact(artifact_path, pipeline=best_pipe, metadata=best_metrics)\n",
    "print(f\"Saved artifact to: {artifact_path}\")\n",
    "\n",
    "# Verify load\n",
    "loaded = load_artifact(artifact_path)\n",
    "print(\"Loaded artifact keys:\", list(loaded.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2433a",
   "metadata": {},
   "source": [
    "## 7) Sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on a small sample\n",
    "pipeline = loaded[\"pipeline\"]\n",
    "sample_X = cleaned_df.drop(columns=[TARGET_COL]).iloc[:6]\n",
    "preds = pipeline.predict(sample_X)\n",
    "prob = pipeline.predict_proba(sample_X)[:, 1] if hasattr(pipeline, \"predict_proba\") else None\n",
    "\n",
    "out = sample_X.copy()\n",
    "out[\"pred\"] = preds\n",
    "if prob is not None:\n",
    "    out[\"proba\"] = prob\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395311c",
   "metadata": {},
   "source": [
    "## 8) Next steps & notes ✅\n",
    "\n",
    "- Run hyperparameter tuning (GridSearchCV / RandomizedSearchCV) for the best model.\n",
    "- Add cross-validation and confidence intervals for metrics.\n",
    "- Feature importance or SHAP explanations to inspect important predictors.\n",
    "- Integrate the saved artifact into the Streamlit `pages/4_Predict.py` for deployment.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides an executable end-to-end flow using existing project helpers. Update configuration values (paths, cleaning strategy, outlier columns, model choice) to iterate further."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
